# Recall Data Synchronizer

## 1. Overview

The Recall Data Synchronizer is a Rust application designed to reliably and efficiently synchronize data from the Recall Competitions App's centralized backend (PostgreSQL and S3-compatible object storage) to the Recall Testnet. It plays a crucial role in making competition-related data available on-chain while decoupling the main Competitions App from the direct complexities and current volatility of the testnet.

## 2. Background and Context

The Recall Competitions App enables agents to participate in various competitions, generating diverse data such as logs, execution traces, and results. To ensure a stable and production-ready environment for the Competitions App, especially while the Recall Testnet is undergoing rapid development and frequent resets, a centralized data storage architecture has been adopted:

- **PostgreSQL:** Serves as the primary store for structured relational data. This includes information about users, agents, competitions, and critically, an index of all objects stored in the S3-compatible storage.
- **S3-Compatible Object Store:** Used for storing larger, unstructured, or semi-structured data blobs generated by participating agents.

Agents interact with the Competitions App API, which handles the writing of data to these centralized stores. The Data Synchronizer then acts as a bridge, moving relevant data from this centralized backend to the Recall Testnet.

## 3. Goal of the Synchronizer

The primary goal of the Recall Data Synchronizer is to:

- **Reliably mirror** relevant data from the centralized S3 object store to the Recall Testnet.
- Utilize the **PostgreSQL `object_index` table** as the source of truth to identify which objects need to be synchronized.
- Operate **resiliently**, capable of handling potential downtime of either the source systems (Postgres, S3) or the Recall Testnet.
- Ensure **data consistency** by picking up synchronization from where it left off after any interruptions.
- Preserve the intended **data structure and format** on the Recall Testnet.

## 4. Core Design & How it Works

The Recall Data Synchronizer operates as an independent service with the following workflow:

1. **Query `object_index`:** Periodically, the synchronizer queries the `object_index` table in the central PostgreSQL database. This table contains metadata about all data objects stored in S3 that are candidates for synchronization. The key piece of information used is the `object_last_modified_at` timestamp (or a similar field indicating updates).
2. **Identify New/Updated Objects:** The synchronizer compares the `object_last_modified_at` timestamps with its own record of the last successful synchronization timestamp. This allows it to identify new or updated objects that need to be processed.
3. **Fetch from S3:** For each new or updated object identified, the synchronizer uses the `object_key` and `bucket_name` (from the `object_index` record) to retrieve the actual data object from the S3-compatible storage.
4. **Write to Recall Testnet:** The retrieved object data is then written to the Recall Testnet. The path/key structure on Recall is designed to mirror the S3 object key structure for consistency.
5. **State Management:** The synchronizer maintains its own internal state (e.g., the timestamp of the last object successfully processed and written to Recall). This state is persisted to ensure that if the synchronizer restarts, it can resume from where it left off, preventing data loss or unnecessary re-processing.

This design ensures that the synchronizer is decoupled from the main application logic and can be scaled or managed independently.

## 5. Data Structures

The synchronizer primarily interacts with the following data structures:

### 5.1. PostgreSQL `object_index` Table

This table is central to the synchronizer's operation. Key columns include:

- `object_key` (TEXT, PK): The full S3 object key (e.g., `competition_id/agent_id/data_type/timestamp.jsonl`).
- `bucket_name` (VARCHAR): The name of the S3 bucket where the object is stored.
- `competition_id` (UUID, FK): Links to the `competitions` table.
- `agent_id` (UUID, FK): Links to the `agents` table.
- `data_type` (VARCHAR): A string identifier for the type of data (e.g., `CHAIN_OF_THOUGHT`, `TRADE_RESULT`, `LOG`).
- `size_bytes` (BIGINT): The size of the object in bytes.
- `content_hash` (VARCHAR): The S3 ETag or another hash of the object's content.
- `object_last_modified_at` (TIMESTAMPTZ): The timestamp indicating when the object was last modified in S3. This is critical for the synchronizer to detect changes.
- `event_timestamp` (TIMESTAMPTZ): Timestamp from the source event, if applicable.

(For the complete schema proposal, refer to `proposal.md`.)

### 5.2. S3 Object Storage Structure

Objects stored in the S3-compatible storage follow a defined hierarchical structure to ensure organization and predictability:

`<bucket_name>/<competition_id>/<agent_id>/<data_type>/<timestamp_or_uuid>.<format_extension>`

Where:

- `<bucket_name>`: As defined in `object_index.bucket_name`.
- `<competition_id>`: UUID of the competition.
- `<agent_id>`: UUID of the agent that generated the data.
- `<data_type>`: Matches `object_index.data_type` (e.g., `chain_of_thought`, `trade_result`, `log`). This helps categorize the data.
- `<timestamp_or_uuid>`: An ISO8601 timestamp (e.g., `2025-05-05T16_30_00Z`) or a unique ID related to the event, ensuring object key uniqueness and allowing chronological sorting if a timestamp is used.
- `<format_extension>`: The file extension indicating the data format (e.g., `jsonl`, `json`, `log`, `bin`).

### 5.3. Synchronization State Management

The synchronizer maintains its own state in a local SQLite database to track:

- **Per-object sync status**: Each object has a status (PendingSync, Processing, Complete)
- **Competition-specific progress**: Separate tracking of the last synced object ID per competition
- **Global progress**: Overall last synced object ID when no competition filter is applied
- **Reset capability**: The synchronizer can clear all state to force re-synchronization

### 5.4. Data Format on Recall Network

When data is written to the Recall Network:

- **Target Structure:** The key/path structure on Recall mirrors the S3 structure: `<competition_id>/<agent_id>/<data_type>/<timestamp_or_uuid>.<format_extension>`.
- **Data Format:**
  - **JSONL (JSON Lines)** is the recommended default for structured data blobs that benefit from line-delimited processing (e.g., sequences of log entries, lists of trade results).
  - Other formats (raw JSON, binary) may be used depending on the `data_type` and `format_extension`.
  - The content of the file on Recall is the raw data fetched from the corresponding S3 object.
- **Ownership:** All data written by the synchronizer to the Recall Network will be owned by the synchronizer's dedicated wallet address. The original `agent_id` is preserved within the key structure.

## 6. Key Technologies

- **Language:** Rust (Stable toolchain)
- **Asynchronous Operations:** Tokio
- **PostgreSQL Interaction:** SQLx (or Diesel, TBD)
- **S3 Interaction:** AWS SDK for Rust (e.g., `aws-sdk-s3`)
- **Recall Testnet Interaction:** A dedicated Recall SDK/Crate (to be specified)
- **Serialization/Deserialization:** Serde
- **Error Handling:** Thiserror, Anyhow
- **Logging:** `tracing` or `log` crate

## 7. Configuration

The application is primarily configured via environment variables. Key variables include:

- `DATABASE_URL`: Connection string for the PostgreSQL database.
- `S3_ENDPOINT_URL`: S3 compatible endpoint URL.
- `S3_ACCESS_KEY_ID`: S3 access key.
- `S3_SECRET_ACCESS_KEY`: S3 secret key.
- `S3_BUCKET_NAME`: Default S3 bucket to read from.
- `RECALL_NODE_URL`: URL for the Recall Testnet node.
- `RECALL_SIGNER_KEY`: Private key for signing transactions on Recall Testnet.
- `SYNC_INTERVAL_SECONDS`: Interval (in seconds) for how often the synchronizer attempts a sync cycle.
- `LOG_LEVEL`: Logging verbosity (e.g., `info`, `debug`, `error`).

Refer to `src/config.rs` (or equivalent) for the definitive list and detailed configuration logic.

## 8. Development & Contribution

### Prerequisites

- Rust toolchain (latest stable version recommended)
- Docker and Docker Compose (for local development environment)
- Git

### Getting Started

1. Clone the repository:

   ```bash
   git clone <repository-url>
   cd recall-synchronizer
   ```

2. Copy the example configuration:

   ```bash
   cp config.example.toml config.toml
   ```

3. Start the development environment:

   ```bash
   docker-compose up -d
   ```

4. Build the project:

   ```bash
   cargo build
   ```

### Development Environment

The Docker Compose setup provides:

- PostgreSQL database for object metadata storage, accessible at `localhost:5432`
  - Username: `recall`
  - Password: `recall_password`
  - Database: `recall_competitions`

### Running the Synchronizer

```bash
cargo run -- --config config.toml
```

Additional options:

- `--reset` - Reset synchronization state (clears all sync records)
- `--competition-id <ID>` - Filter by competition ID (maintains separate progress per competition)
- `--since <TIMESTAMP>` - Synchronize data since timestamp (RFC3339 format)
- `--verbose` - Show verbose output

#### Reset Functionality

The `--reset` flag clears all synchronization state, allowing you to re-sync all data. This is useful when:

- The Recall network has been reset or data has been lost
- You need to force re-synchronization of all objects
- Testing synchronization from a clean state

Example:

```bash
# Reset and re-sync all data
cargo run -- --config config.toml --reset
```

### Docker Compose Commands

- Start services: `docker-compose up -d`
- Stop services: `docker-compose down`
- View logs: `docker-compose logs -f postgres`
- Reset database: `docker-compose down -v && docker-compose up -d`

### Running Tests

The project provides several test targets with different configurations:

#### Test Targets

- `make test-fast` - Run tests with fake implementations only (no Docker required)
- `make test` - Run tests with real PostgreSQL and SQLite implementations
- `make test-integration` - Run tests with custom configuration (respects env vars)
- `make test-coverage` - Run tests with coverage reporting

#### Test Configuration

Tests can be configured via environment variables that override `test_config.toml`:

- `ENABLE_DB_TESTS=true/false` - Enable tests with real PostgreSQL database
- `ENABLE_SQLITE_TESTS=true/false` - Enable tests with real SQLite storage
- `ENABLE_S3_TESTS=true/false` - Enable tests with real S3 (not yet implemented)
- `ENABLE_RECALL_TESTS=true/false` - Enable tests with real Recall network (not yet implemented)

Example usage:

```bash
# Run only with fake implementations
make test-fast

# Run with real PostgreSQL and SQLite
make test

# Run with only PostgreSQL enabled
ENABLE_DB_TESTS=true ENABLE_SQLITE_TESTS=false make test-integration

# Run with custom configuration
ENABLE_DB_TESTS=true cargo test
```

#### Testing Philosophy

This project follows these testing principles:

1. **Trait-Based Abstraction**: External dependencies like S3, databases, and the Recall network are represented by traits with well-defined interfaces.

2. **In-Memory Fake Implementations**: Each trait has a lightweight, in-memory fake implementation for testing, alongside its real implementation.

3. **Parameterized Testing**: Tests run against both implementations using a factory pattern:

   - Regular tests run with fake implementations by default (fast and reliable)
   - The same tests can optionally run against real implementations when configured with environment variables

4. **Special Testing Methods**: Fake implementations have special methods with the `fake_` prefix for simulating edge cases and failure scenarios.

### Project Structure

- `src/db/` - Database abstraction for reading object metadata

  - `database.rs` - Database trait definition
  - `models.rs` - Data models
  - `fake.rs` - In-memory implementation for testing
  - `postgres.rs` - PostgreSQL implementation

- `src/sync/storage/` - Storage abstraction for synchronization state
  - `sync_storage.rs` - SyncStorage trait definition
  - `models.rs` - Data models
  - `fake.rs` - In-memory implementation for testing
  - `sqlite.rs` - SQLite implementation

### Code Quality & Formatting

Ensure your code adheres to the project's standards:

- **Formatting:** `cargo fmt --all -- --check`
- **Linting:** `cargo clippy --all-targets --all-features -- -D warnings`
- **Documentation:** `cargo doc --no-deps --all-features`
- **Dependency Audit:** `cargo audit` (requires `cargo-audit` to be installed: `cargo install cargo-audit`)

Please refer to the following documents in the `ai/docs/` directory for comprehensive guidelines:

- `org-general-practices.mdc`
- `org-rust-standards.mdc`
- `repo-specific-config.mdc`

---

This document should be updated as the project evolves.
